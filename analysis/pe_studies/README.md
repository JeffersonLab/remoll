# Known glitches on UMass Mocha cluster:
* /share/library will refuse to show up unless you qlogin to compute-0-0 and ls it directly
* Similarly, you need to log in to compute-0-0 and execute remoll at least once by hand to get all the libraries in place

Once remoll is built add these two lines into bin/remoll.sh to get it to work properly:

`export PATH="/share/library":${PATH}`

`export LD_LIBRARY_PATH="/share/library/":${LD_LIBRARY_PATH}`

- - - 

# Available Detector Optimization Analyses

## 'scripts' Folder
The 'scripts' folder contains any bash scripts that greatly simplify the job of scanning parameters of various rings and optimizing detectors after any change in upstream electron transport, and serve as a template for further studies that can be done.

Each script within the scripts folder is intended to be executed from the pe_studies folder -> `./scripts/full-scan.sh [args]`, and each of them has been set up so that all of the relative paths and needed executables are automatically referenced correctly or moved into the correct places (and if not, then a useful message is printed explaining what went wrong and how to fix it by hand).

All of the scripts assume that you are using the UMass Mocha compute cluster's SGE qsub system. This system has some issues (listed above), which have had semi-reasonable work-arounds developed that do work (but require additional effort to get right).

'scripts' contents:
### produce.sh
Usage `./scripts/produce.sh numberRepeat (no path, just name)macroName.mac nameStub cadp.csv`

This script generates the flux at the downstream "flat" idealized detector array (where each individual sub-segment of each detector has been flattened and projected onto the x-y axis, and all detectors are separate, but with the same numbering system as the full detector system), which allows later sampling from that accepted flux (see full-scan.sh)

The arguments are as follows:
* numberRepeat: "50" is a good default and is the number of separate simulation jobs to execute. Ideally we want around 5 million events for the main generators to get reasonable statistics, but this takes days if done in just one job. Splitting that into 50 jobs of 100k events each (the default contained in the macros/runexample_...parallel.mac macros) is ideal.
* macroName.mac: "runexample_moller_parallel.mac" (or epelastic or epinelastic) are the pre-written macros which should work. Note that these macros depend on the "flat" parallel world gdml file, which is also generated by this script and copied into the correct place. These sample macros are adequate, though please note which magnetic fields are used, as well as the GDML geometry, raster selection, and kryptonite settings. These choices were somewhat arbitrary when the macros were committed.
* nameStub: "moller" (or epelastic or epinelastic) are what you should probably use here. This field is just to avoid having to parse whatever macro's name is given and is used as the remollout_NAME.root output.
* cadp.csv: "cadp.csv" is the usual name of the csv file that will be used in the perl scripts in the remoll-detector-generator scripts. The file is assumed to live inside that repository, and that repository is assumed to be parallel to the current install of remoll (three levels up).

In order for the flux that is produced to be useful, edit mollerParallel.gdml to only have the detectors you want. The flat parameterized detector from the detector perl scripts should be used to interface with the full-scan.sh.

Once you have generated the outputs, please make sure that none of the individual jobs have segfaulted or failed in any way. If they have then just go and manually re-run that failed job.

Once all of the jobs are done the easiest thing to do is execute `hadd -f moller.root out_moller_*/remollout_parallel_moller.root`, which merges them all into one. Because of the way that remoll samples and assigns relative weights, it is important for you to know the uA beam current that was assumed while simulating and the number of individual jobs that were `hadd` together. The rates in any one remoll job are self normalized so that that one job stands alone. `hadd` multiple jobs together will then multiply the rates higher by the multiplicity of separate jobs. Similarly if you want the rate in Hz/uA you need to later divide by the number of uA (which is 65 uA in the sameple macros given).

### scan.sh
Usage: `./scripts/scan.sh fixed-non-scanned "variable-to-scan" min-of-scan (-30) max-of-scan (30) step-size (0.5) Reflectivity Cerenkov Scintillation z_position Pass Detector`

This script assumes that a "Geometry name" suffix'ed set of GDML files has been placed in the geometry_sandbox folder of remoll, and that the user wants to scan shooting at different positions or angles onto the light guide. The angles, x position, and z position of the beam generator are passed in as arguments. Practically speaking this script needs to just be called from within the `scripts/full-scan.sh` script, which knows to produce the GDML files itself and read the cadp.csv file to know what x and z points to shoot the beam from. The analysis requires 2 passes, to simulate and then analyze the sim outputs. And the outputs come in the scans.root output file that contains several metrics, including average number of pes and it's RMS, as well as number of bounces against the reflector, lightguide, and both, as well as the input command line parameters.

The arguments are as follows:
* fixed-non-scanned: "0.0" is the default number here, which decides the fixed value of the fixed "variable-to-scan" determined in the next argument.
* variable-to-scan: Typing "angle" or "x" here refers to whether the scan is over "angle" or "x", with the other variable held fixed, at the value given in the previous argument. Angle is in degrees, and x is in cm.
* min-of-scan: "-30" is the default number here, which is the minimum value of the scanned variable (in degrees, or cm, if angle or x is scanned, respectively).
* max-of-scan: "30" is similarly the max.
* step-size: "0.5" is the default step size, and determines the steps between min and max (can be any double).
* Reflectivity: "0.9" is the default reflectivity. This variable edits the material properties in the gdml files before beginning the simulation.
* Cerenkov: "1.0" is a boolean (it's bash... 1.0 means it is turned on, and 0.0 means it is turned off. Really anything other than 0.0 will turn it on) to edit the macro file and turn cerenkov on or off. The Macro is assumed to be macros/preserve_scans.mac (and can be editted by the user if wanted.)
* Scintillation: "1.0" is a boolean (it's bash... 1.0 means it is turned on, and 0.0 means it is turned off. Really anything other than 0.0 will turn it on) to edit the macro file and turn scintillation on or off. The Macro is assumed to be macros/preserve_scans.mac (and can be editted by the user if wanted.)
* z position of the beam origin: "0" is the default starting point, defining the origin of the beam (plus raster) that is used to spray the detector. This number will be edited within the script to adjust for the angle of the light guide and reflector so that the beam starts as close to the detector as possible. This number should be determined empirically using the macros/detector_vis.mac macro (probably needs to be in the higher remoll macros folder to get picked up correctly). The full-scan.sh and ref-scan.sh scripts have their own ways of determining what this number should be (also was determined partially empirically).
* Geometry name: "Mainz" or "R5o" or "R1", etc. are names that are used to determing the GDML files used (as well as the detector number based on ring name, though det number is the final paramter that can be passed in by hand).
* Pass: "1" or "2". "1" means it is the first pass, when the simulation jobs are submitted. "2" means the `pe` analysis pass, where the outputs are analyzed (and if pass 1 segfaulted it will resubmit the job. This is convenient for relatively small numbers of jobs, but when it gets to the thousands something better like the sub-ref-scan.sh approach is preferable).
* detector: "540210" is the detector number, which can be determined automatically if the "Geometry name" input was a ring number ("R1", "R2" ... "R5o", "R6"), but must be passed by hand for unique GDML geometry names.
### full-scan.sh
This is a script that wraps around scan.sh to produce a lookup table's worth of angle and light guide length (x) hits. The results can be `hadd` together to be fed as input into the `scripts/do_all_rates.sh` calculation that generates the `scripts/produce.sh` convoluted signal and background results.

### do_all_rates.sh
This script assumes several files are already in place and will complain if they are not in the right places or with the right names. Do `scripts/produce.sh` and `scripts/full-scan.sh` first to properly prepare for getting the lightguide backgrounds lookup table folded into the MOLLER flux for final numbers. Please note that the "signal" size has been assumed at 12 PEs vs 25 PEs for rings 1,2,3,4, and 6, vs 5, respectively. The "background" signal is entirely determined by the lookup table coming from the `scripts/full-scan.sh` analysis.
### ref-scan.sh
This is a script that will allow you to scan the reflector length and reflector angle parameters to determine the ideal size and angle of the reflector segment for ideal light collection. Parameters for cerenkov and scintillation can be turned on and off to help understand what is happening. With some care, the relflectivity of the reflector section and the light guide section could be separated from eachother and this can be used to determine the impact of blackening in the light guide section (you would need to edit the material property names or values by hand in the GDML files copied into this analysis in order for that test to be feasible, though it has been done before. See the logic surrounding the Reflectivity parameter in these scripts for guidance on how to achieve this)
This script is run similarly to scan.sh. For instructions on general parameters to enter, input ./scripts/ref-scan.sh
### sub-ref-scan.sh
This script executes similarly to `scripts/ref-scan.sh`, but it loops over sub-segments of the reflector, allowing fine tuning of a fresnel-type mirror, for optimal light collection. This is just one easiest way that the light guides and reflectors can be studied in addition to the default flat segments we have started with. 
### getMax.C
This script assumes you have produced a sub-ref-scan.sh analysis (which by the way takes a really long time, like 4 days worth of simulation time at 30 jobs at a time). Once that analysis is done you should hadd all of its scans.root files into one single file and feed that file into the getMax.C scripst as the first argument. The second argument should be a double == to the value of the mean PE counts when there is NO reflector section at all (i.e. it is the amount of PEs from photons that never hit the reflector section in the first place, which are duplicate counted for each independent simulation of sub-segments).

Setting up a no-reflector simulation should be as simple as taking one of the many job outputs from sub-ref-scan.sh and changing the geometry to comment out the reflector_540210 (or whatever detector number) component (not reflectorSkin though, that is the other-walls of the reflector section).

## Look up table prediction of signal to background ratios in MOLLER detectors ===
* First, produce a flux at the detector array to sample from
    * Run scripts/produce.sh for moller, epelastic, and epinelastic - 5 million events (50x 100k jobs) is a good high statistics goal
    * Note that mocha.physics.umass.edu often segfaults jobs, and so you may need to redo some of them by hand
    * Then hadd moller.root out\_moller\_\*/remollout\_\*.root
    * Again for epelastic and epinelastic
    * Move these sample files to the folder of choice for doing subsequent analysis
* Run scripts/full-scan.sh to create a look up table of light guide PE yields for the geometry
* Then run bkgd_pe on the flux sample root files you have to generate outputs


## Final notes
Always check for segfault failed runs before proceeding.
Delete core-dump failed rootfiles and when confident with results also delete the root files systematically to save disk space.
